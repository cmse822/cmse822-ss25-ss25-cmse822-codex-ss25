[GravityCollapse] Using gravity solver method = cooley_tukey
[GravityCollapse] Gravity solver completed in 29.0796 seconds.
[GravityCollapse] Uniform-sphere potential check:
   L₁ = 7.80921e+14, L₂ = 8.53079e+14 (over 134217728 cells)
Gravity is ENABLED, G: 6.67e-08
CFL=0.5, sound_crossings=0.0004
Initial max wave speed= 0.748331, crossingTime= 2.67261e+11, totalTime= 1.06904e+08
HDF5 file written to: agoge_init.h5
HDF5 file written to: agoge_final.h5
Simulation finished. Final time=1.06904e+08, step count=1
Final data written to agoge_final.h5

================= PERFORMANCE REPORT =================
Timer Name              Total Time (s)     Calls     Avg (ms)Mega Zone Updates/s
--------------------------------------------------------------------------
main                         98.237501         1 98237.500948            1.37
timeLoop                     30.591011         1 30591.010800            4.39
solvePoisson                 29.471950         1 29471.949524            4.55
--------------------------------------------------------------------------

Elapsed Time: 98.731s
    SP GFLOPS: 0.000
    DP GFLOPS: 2.579
    x87 GFLOPS: 0.000
    CPI Rate: 0.704
    Average CPU Frequency: 3.630 GHz
    Total Thread Count: 1
Effective CPU Utilization: 2.3%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 0.928 out of 40
Memory Bound: 6.3% of Pipeline Slots
    Cache Bound: 25.6% of Clockticks
     | A significant proportion of cycles are being spent on data fetches from
     | caches. Check Memory Access analysis to see if accesses to L2 or L3
     | caches are problematic and consider applying the same performance tuning
     | as you would for a cache-missing workload. This may include reducing the
     | data working set size, improving data access locality, blocking or
     | partitioning the working set to fit in the lower cache levels, or
     | exploiting hardware prefetchers. Consider using software prefetchers, but
     | note that they can interfere with normal loads, increase latency, and
     | increase pressure on the memory system. This metric includes coherence
     | penalties for shared data. Check Microarchitecture Exploration analysis
     | to see if contested accesses or data sharing are indicated as likely
     | issues.
     |
    DRAM Bound: 19.1% of Clockticks
        DRAM Bandwidth Bound: 0.0% of Elapsed Time
    NUMA: % of Remote Accesses: 0.0%

    Bandwidth Utilization
    Bandwidth Domain             Platform Maximum  Observed Maximum  Average  % of Elapsed Time with High BW Utilization(%)
    ---------------------------  ----------------  ----------------  -------  ---------------------------------------------
    DRAM, GB/sec                 224                         31.900    2.746                                           0.0%
    DRAM Single-Package, GB/sec  112                         29.700    3.207                                           0.0%
Vectorization: 44.5% of Packed FP Operations
 | A significant fraction of floating point arithmetic instructions are scalar.
 | This indicates that the code was not fully vectorized. Use Intel Advisor to
 | see possible reasons why the code was not vectorized.
 |
    Instruction Mix
        SP FLOPs: 0.0% of uOps
            Packed: 0.0% from SP FP
                128-bit: 0.0% from SP FP
                256-bit: 0.0% from SP FP
                512-bit: 0.0% from SP FP
            Scalar: 0.0% from SP FP
        DP FLOPs: 20.6% of uOps
            Packed: 44.5% from DP FP
                128-bit: 44.5% from DP FP
                 | Using the latest vector instruction set can improve
                 | parallelism for this code. Consider either recompiling the
                 | code with the latest instruction set or using Intel Advisor
                 | to get vectorization help.
                 |
                256-bit: 0.0% from DP FP
                512-bit: 0.0% from DP FP
            Scalar: 55.5% from DP FP
             | A significant fraction of floating point arithmetic instructions
             | are scalar. This indicates that the code was not fully
             | vectorized. Use Intel Advisor to see possible reasons why the
             | code was not vectorized.
             |
        x87 FLOPs: 0.0% of uOps
        Non-FP: 79.4% of uOps
    FP Arith/Mem Rd Instr. Ratio: 2.029
    FP Arith/Mem Wr Instr. Ratio: 3.067
Collection and Platform Info
    Application Command Line: ../../agoge_run "../../problems/GravityCollapse-512-Gr.yaml" 
    Operating System: 5.15.0-126-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.5 LTS"
    Computer Name: dev-intel18
    Result Size: 152.4 MB 
    Collection start time: 03:04:26 09/02/2025 UTC
    Collection stop time: 03:06:04 09/02/2025 UTC
    Collector Type: Driverless Perf per-process sampling
    CPU
        Name: Intel(R) Xeon(R) Processor code named Skylake
        Frequency: 2.394 GHz
        Logical CPU Count: 40
        Max DRAM Single-Package Bandwidth: 112.000 GB/s
        LLC size: 28.8 MB 

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
