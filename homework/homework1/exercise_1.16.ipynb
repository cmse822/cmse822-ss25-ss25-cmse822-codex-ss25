{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.16\n",
    "\n",
    "#### Brynn Roy\n",
    "1/30/25\n",
    "\n",
    "**Exercise 1.16**. Consider the following pseudocode of an algorithm for summing $ùëõ$ numbers $ùë•[ùëñ]$ where $ùëõ$ is a power of $2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for s=2,4,8,...,n/2,n:\n",
    "    for i=0 to n-1 with steps s:\n",
    "        x[i] = x[i] + x[i+s/2]\n",
    "sum = x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analyze the spatial and temporal locality of this algorithm, and contrast it with the standard algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i=0,1,2,...,n-1\n",
    "    sum = sum + x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 1 Spacial Locality**: The first algorithm has a set up where the step size of the summation algorithm increase with each outer loop. As $s$ increases, the algorithm's memory traffic cost increases as it is reaching for memory further and further from the most recent data. This means the algorithm does not have good spacial locality. \\\n",
    "**Algorithm 1 Temporal Locality**: This algortihm displays poor temporal locality because of the repeated use of variables as $s$ is small. The inner loop repeats over the length of the vector with a step of $s$, which when $s$ is small, this causes the computer to be accessing many values of the vector $x$. For example, the $x[0]$ is accessed at the start of every single inner loop, but if the vector is long, then $x[0]$ has likely been removed from cache. Additionally, as $s$ increases, the temporal locality improves as $s$ begins to repeatedly access large even values that have been recently accessed. However, with a large $x$ vector, the temporal locality would take a long time to improve. \n",
    "\n",
    "**Algorithm 2 Spacial Locality**: The second algorithm has much better spacial locality than the first. This is because the for loop accesses values that are stored next to each other in sucessive order. By pulling data in this order, the algorithm's memory traffic is much lower. Furthermore, the memory traffic cost will not significantly scale with different sizes of $x$ because it will always go in order, unlike the first algorithm. \\\n",
    "**Algorithm 2 Temporal Locality**: By comparison to the first algorithm, this algorithm also improves on temporal locality. For the second algorithm, the only variable being accessed repeatedly is $sum$ on every loop. This ensures that the $sum$ will remain in cache, therefore decreasing the temporal locality. By contrast, the first algorithm was accessing the same variable at the start of each inner loop, which was not guaranteed to be in cache. \n",
    "\n",
    "*This means the second algorithm has better spacial and temporal locality.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
