{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "**Team**: Thread Titans\n",
    "\n",
    "**Team Members**: Alex, Bashar, Fizza, Maya, Ramin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1\n",
    "\n",
    "Let us compare the speed of a classical FPU, and a pipelined one. Show that the result rate is now dependent on $n$: give a formula for $r(n)$, and for $r(\\infty) = \\lim_{n \\rightarrow \\infty}r(n)$. What is the asymptotic improvement in $r$ over the non-pipelined case? Next you can wonder how long it takes to get close to the asymptotic behavior. Show that for $n = n_{1/2}$ you get $r(n) = r_\\infty/2$. This is often used as the definition of $n_{1/2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answer\n",
    "\n",
    "The result rate $r(n)$ is is defined as the reciprocal of the average time per result for  $n$ results:\n",
    "$$\n",
    "r(n) = \\frac{n}{t(n)}\n",
    "$$\n",
    "For a pipelined processor, substituting $t(n)$ for a pipelined FPU gives the result rate as:\n",
    "$$\n",
    "r(n) = \\frac{n}{[s + \\ell + n - 1] \\tau}\n",
    "$$\n",
    "Unlike the case for the traditional FPU, the result rate is now dependent on $n$ aswell as the pipeline setup.\n",
    "\n",
    "The asymptotic rate as $n \\to \\infty$ is:\n",
    "$$\n",
    "r_\\infty = \\lim_{n \\to \\infty} r(n) = \\lim_{n \\to \\infty} \\frac{n}{[s + \\ell + n - 1] \\tau}.\n",
    "$$\n",
    "\n",
    "For a very or large $n$, the $n$ term dominates in the denominator:\n",
    "$$\n",
    "r_\\infty = \\lim_{n \\to \\infty}\\frac{n}{n \\tau} = \\frac{1}{\\tau}\n",
    "$$\n",
    "\n",
    "The asymptotic improvement is computed by dividing the result rate for pipelined FPU with the traditional FPU as $n$ goes to infinity.\n",
    "\n",
    "$$ \\text{Improvement} = \\frac{r_{pipeline}(\\infty)}{r_{traditional}} = \\frac{1/\\tau}{1/\\ell\\tau} = \\ell$$\n",
    "\n",
    "Therefore, a pipelined FPU gives $\\ell$-fold improvement.\n",
    "\n",
    "To know about how quickly we can approach the asymptotic behaviour, we know already that\n",
    "\n",
    "$$\n",
    "r(n) = \\frac{n}{[s + \\ell + n - 1] \\tau},\\;\n",
    "r(\\infty) = \\frac{1}{\\tau}\n",
    "$$\n",
    "By definition, at $n = n_{1/2}$, the result rate $r(n)$ is half of its asymptotic value.\n",
    "$$\n",
    "r(n_{1/2}) = \\frac{r(\\infty)}{2} \n",
    "$$\n",
    "Substituting the formulas:\n",
    "$$\n",
    "\\frac{n_{1/2}}{[s + \\ell + n_{1/2} - 1] \\tau} = \\frac{1}{2} \\cdot \\frac{1}{\\tau}\n",
    "$$\n",
    "Making $n_{1/2}$ the subject:\n",
    "$$\n",
    "2 n_{1/2} = s + \\ell + n_{1/2} - 1\n",
    "$$\n",
    "$$\n",
    "n_{1/2} = s + \\ell  - 1\n",
    "$$\n",
    "Thus $n_{1/2}$ i.e number needed of results needed for the result rate $r(n)$ to reach half of the maximum rate $r(\\infty)$ is the sum of the setup cost and the number of stages. After this many steps,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2\n",
    "\n",
    "Analyze the speedup and $n_{1/2}$ of linked triads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answer\n",
    "In a serial pipeline, when performing an operation like $a_i \\leftarrow b_i + c_i \\cdot d_i$, the multiplication and addition operations are executed sequentially. Assuming the multiplication requires $\\ell$ stages, followed by the addition, which also requires $\\ell$ stages, the total time for this operation for $n$ results is:\n",
    "$$\n",
    "t_{\\text{serial}}(n) = 2 n \\cdot \\ell \\cdot \\tau.\n",
    "$$\n",
    "The result rate is:\n",
    "$$\n",
    "r_{\\text{serial}}(n) = \\frac{1}{2 \\ell \\tau}.\n",
    "$$\n",
    "\n",
    "In a linked triad, the multiplication and addition operations are fused into a single pipeline, allowing the multiplication result to directly feed the addition without storing the intermediate result in memory. This reduces the time to ideally $\\ell \\cdot \\tau$, but in practice, it may require $\\ell + e$ stages, where $e$ accounts for additional overhead. The total time for $n$ results in the linked triad pipeline is:\n",
    "$$\n",
    "t_{\\text{linked}}(n) = (s + \\ell + e + n - 1) \\cdot \\tau.\n",
    "$$\n",
    "The result rate is:\n",
    "$$\n",
    "r_{\\text{linked}}(n) = \\frac{n}{(s + \\ell + e + n - 1) \\cdot \\tau}.\n",
    "$$\n",
    "\n",
    "Asymptotically, as $n \\to \\infty$, the dominant term in the denominator becomes $n$, so the asymptotic rate simplifies to:\n",
    "$$\n",
    "r_{\\text{linked}}(\\infty) = \\lim_{n \\to \\infty} \\frac{n}{(s + \\ell + e + n - 1) \\cdot \\tau} = \\frac{1}{\\tau}.\n",
    "$$\n",
    "\n",
    "The speedup of is computed as the ratio of the asymptotic result rates:\n",
    "$$\n",
    "\\text{Speedup} = \\frac{r_{\\text{linked}}(\\infty)}{r_{\\text{serial}}(\\infty)} = \\frac{\\frac{1}{\\tau}}{\\frac{1}{2 \\ell \\tau}} = 2 \\ell.\n",
    "$$\n",
    "\n",
    "Therefore, the linked triad pipelined processor is asymptotically $2 \\ell$ times faster than the non-pipelined serial processor for these type of instructions.\n",
    "\n",
    "To calculate $n_{1/2}$, where the result rate is half of the asymptotic rate, we solve:\n",
    "$$\n",
    "r_{\\text{linked}}(n_{1/2}) = \\frac{r_{\\text{linked}}(\\infty)}{2}.\n",
    "$$\n",
    "Substituting the formula for $r_{\\text{linked}}(n)$:\n",
    "$$\n",
    "\\frac{n_{1/2}}{(s + \\ell + e + n_{1/2} - 1) \\cdot \\tau} = \\frac{\\frac{1}{\\tau}}{2}.\n",
    "$$\n",
    "Simplifying:\n",
    "$$\n",
    "\\frac{n_{1/2}}{s + \\ell + e + n_{1/2} - 1} = \\frac{1}{2}.\n",
    "$$\n",
    "Rearranging gives:\n",
    "$$\n",
    "2 n_{1/2} = s + \\ell + e + n_{1/2} - 1,\n",
    "$$\n",
    "and solving for $n_{1/2}$:\n",
    "$$\n",
    "n_{1/2} = s + \\ell + e - 1.\n",
    "$$\n",
    "\n",
    "Thus, $n_{1/2}$ depends on the setup cost $s$, the pipeline depth $\\ell$, and any additional overhead $e$ introduced by the fused operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1.3\n",
    "\n",
    "Analyze the speedup and $n_{1/2}$ of a processor with multiple pipelines that operate in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answer\n",
    "\n",
    "Suppose there are $p$ independent pipelines, each capable of processing a stream of operands. With $p$ parallel pipelines, the workload is divided equally among the pipelines. Each pipeline handles approximately $\\frac{n}{p}$ results. The total time for all pipelines to complete their respective workloads is:\n",
    "$$\n",
    "t_{\\text{parallel}}(n) = (s + \\ell + \\frac{n}{p} - 1) \\cdot \\tau.\n",
    "$$\n",
    "The result rate is:\n",
    "$$\n",
    "r_{\\text{parallel}}(n) = \\frac{n}{t_{\\text{parallel}}(n)} = \\frac{n}{(s + \\ell + \\frac{n}{p} - 1) \\cdot \\tau}.\n",
    "$$\n",
    "\n",
    "Asymptotically, as $n \\to \\infty$, the dominant term in the denominator of $t_{\\text{parallel}}(n)$ is $\\frac{n}{p}$, so the asymptotic result rate is:\n",
    "$$\n",
    "r_{\\text{parallel}}(\\infty) = \\lim_{n \\to \\infty} \\frac{n}{(s + \\ell + \\frac{n}{p} - 1) \\cdot \\tau} = \\frac{n}{\\frac{n}{p} \\cdot \\tau} = p \\cdot \\frac{1}{\\tau}.\n",
    "$$\n",
    "\n",
    "The speedup compared to a classical FPU is the ratio of the asymptotic result rates:\n",
    "$$\n",
    "\\text{Speedup} = \\frac{r_{\\text{parallel}}(\\infty)}{r_{\\text{classical}}(\\infty)} = \\frac{p \\cdot \\frac{1}{\\tau}}{\\frac{1}{\\ell \\tau}} = p \\ell.\n",
    "$$\n",
    "\n",
    "If we compare this parallel pipelined processor  to the single pipelined one, then the speedup is ratio of asymptotic result rates:\n",
    "$$\n",
    "\\text{Speedup} = \\frac{r_{\\text{parallel}}(\\infty)}{r_{\\text{single}}(\\infty)} = \\frac{p \\cdot \\frac{1}{\\tau}}{\\frac{1}{\\tau}} = p.\n",
    "$$\n",
    "\n",
    "To calculate $n_{1/2}$, we solve for the number of results needed to achieve half the asymptotic result rate. By definition:\n",
    "$$\n",
    "r_{\\text{parallel}}(n_{1/2}) = \\frac{r_{\\text{parallel}}(\\infty)}{2}.\n",
    "$$\n",
    "Substituting the formula for $r_{\\text{parallel}}(n)$:\n",
    "$$\n",
    "\\frac{n_{1/2}}{(s + \\ell + \\frac{n_{1/2}}{p} - 1) \\cdot \\tau} = \\frac{p \\cdot \\frac{1}{\\tau}}{2}.\n",
    "$$\n",
    "Simplifying:\n",
    "$$\n",
    "\\frac{n_{1/2}}{s + \\ell + \\frac{n_{1/2}}{p} - 1} = \\frac{p}{2}.\n",
    "$$\n",
    "Rearranging gives:\n",
    "$$\n",
    "2 n_{1/2} = p \\cdot (s + \\ell + \\frac{n_{1/2}}{p} - 1).\n",
    "$$\n",
    "Expanding and simplifying:\n",
    "$$\n",
    "2 n_{1/2} = p(s + \\ell - 1) + n_{1/2}.\n",
    "$$\n",
    "Solving for $n_{1/2}$:\n",
    "$$\n",
    "n_{1/2} = p(s + \\ell - 1).\n",
    "$$\n",
    "\n",
    "Thus, the $n_{1/2}$ depends linearly on the number of pipelines $p$, the setup cost $s$, and the pipeline depth $\\ell$. After this many steps, the rate converges to half the asymptotic rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.4 \n",
    "The operation \n",
    "```C\n",
    "    for (i){\n",
    "        x[i+1] = a[i]*x[i] + b[i];\n",
    "    }\n",
    "``` \n",
    "can not be handled by a pipeline because there is a dependency between input of one iteration of the operation and the output of the previous. However, you can transform the loop into one that is mathematically equivalent, and potentially more efficient to compute. Derive an expression that computes ```x[i+2]``` from ```x[i]``` without involving ```x[i+1]```. This is known as recursive doubling. Assume you have plenty of temporary storage. You can now perform the calculation by \n",
    "1. Doing some preliminary calculations\n",
    "2. computing x[i],x[i+2],x[i+4],..., and from these,\n",
    "3. compute the missing terms x[i+1],x[i+3],....\n",
    "Analyze the efficiency of this scheme by giving formulas for T0(n) and Ts(n). Can you\n",
    "think of an argument why the preliminary calculations may be of lesser importance in\n",
    "some circumstances ?\n",
    "---\n",
    "\n",
    "**Answer** : The initial expression is given as \n",
    "$$ x_{i+1} = a_i \\,x_i + b_i $$\n",
    "\n",
    "Replacing $x_{i+1}$ from the above and calculating $x_{i+2}$ gives \n",
    "$$ x_{i+2} = a_{i+1} (a_i x_i + b_i) + b_{i+1} $$\n",
    "\n",
    "We notice by replacing $i\\rightarrow i+1$\n",
    "$$x_{i+3} = a_{i+2} (a_{i+1} x_{i+1} + b_{i+1}) + b_{i+2}$$\n",
    "\n",
    "Now let us assume we have computed $x_0,x_2,x_4,\\dots,x_{2n}$. Despite this, we observe that the computation remains sequential, as each step still depends on the result of the previous one, preventing pipeline to be saturated. So the complexity of this is similar to initial loop (with an additional calculation), but for half of the data.\n",
    "\n",
    "Next, we notice for calculating the missing parts which are $x_1,x_3,x_4,\\dots,x_{2n+1}$ we have\n",
    "$$ x_1 = a_0 x_0 + b_0$$\n",
    "$$ x_3 = a_2 x_2 + b_2$$\n",
    "$$ \\vdots$$\n",
    "$$ x_{2n+1} = a_{2n} x_{2n} + b_{2n} $$\n",
    "Therefore, now it is possible to saturate the pipeline (assuming we have plenty of memory and we have stored previously computed values). \n",
    "\n",
    "We can see for the first half of data we can't scale computation by utilizing pipeline, but for the second half it is possible. If the scale of computation for sufficiently large $n$ is approximately $\\ell$ and assuming there is still enough capacity in pipline to damp the overhead of extra calculations (going from $i$ to $i+2$ we are doing some extra calculation) then we can say :\n",
    "$$ T_s(n) = T_0(\\frac{n}{2}) + \\frac{1}{\\ell} T_0(\\frac{n}{2})$$\n",
    "\n",
    "Now instead of going forward by two steps, we can go forward by $j$ steps and therefore we have :\n",
    "$$ T_s(n) = T_0(\\frac{n}{j}) + \\frac{j-1}{\\ell} T_0(\\frac{n}{j})$$\n",
    "\n",
    "Ofcourse, value of $j$ depends on the capacity of pipeline since going forward with larger steps requires more computation. However, in this way we can see we are lowering the effects of initial calculations on the overall time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.5\n",
    "\n",
    "The L1 cache is smaller than the L2 cache, and if there is an L3, the L2 is smaller than the L3. Give a practical and a theoretical reason why this is so ?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer** : From a theoretical point of view one can say \n",
    "the hierarchy of cache memory (L1, L2, and L3) is designed to balance speed, cost, and efficiency in modern computer systems. The cache hierarchy is designed based on the principle of locality\n",
    "1. programs tend to access a small portion of memory repeatedly (temporal locality) \n",
    "2. programs access nearby memory locations (spatial locality).\n",
    "\n",
    "The L1 cache is optimized to exploit **temporal locality** by storing the most frequently accessed data. The L2 and L3 caches are progressively larger to exploit **spatial locality** and store larger working sets of data.\n",
    "Smaller caches (like L1) use faster but more expensive technologies (like SRAM). Larger caches (like L3) cheaper technologies but with higher latency. Therefore, by organizing caches in a hierarchy, the system can balance performance and cost. The smaller, faster caches handle the most critical data, while the larger, slower caches reduce the need to access main memory, which is much slower.\n",
    "\n",
    "\n",
    "From a practical perspective, the L1 cache is typically integrated directly into the CPU core to minimize latency. This limits its size due to space constraints on the CPU die.\n",
    "The L2 cache is often shared among a few cores or located nearby, allowing it to be larger than L1 but still relatively fast.\n",
    "The L3 cache is shared across all cores and is located farther from the CPU cores, enabling it to be much larger but with higher latency. \n",
    "Smaller caches (L1) use more expensive memory technologies to achieve low latency. Larger caches (L2 and L3) use cheaper, denser memory technologies to keep manufacturing costs manageable.\n",
    "Increasing the size of L1 cache would significantly increase the cost and power consumption of the CPU, making it impractical for most applications.\n",
    "Faster caches (like L1) generate more heat due to their high speed and frequent access. Keeping the L1 cache small helps manage heat dissipation and prevents thermal throttling.\n",
    "Larger caches (like L3) operate at lower speeds and generate less heat per bit, making them more suitable for larger sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.11\n",
    "The recursive doubling algorithm for summing the elements of an array is:\n",
    "```{python}\n",
    "for (s=2; s<2*n; s*=2)\n",
    "    for (i=0; i<n-s/2; i+=s)\n",
    "        x[i] += x[i+s/2]\n",
    "```\n",
    "Analyze bank conflicts for this algorithm. Assume $n = 2^p$ and banks have $2^k$ elements where $k < p$. Also consider this as a parallel algorithm where all iterations of the inner loop are independent, and therefore can be performed simultaneously.\n",
    "Alternatively, we can use recursive halving:\n",
    "```{python}\n",
    "for (s=(n+1)/2; s>1; s/=2)\n",
    "    for (i=0; i<n; i+=1)\n",
    "        x[i] += x[i+s]\n",
    "```\n",
    "Again analyze bank conficts. Is this algorithm better? In the parallel case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Yes, the later algorithm has less bank conflicts in the parallel case. Let's analyze it with an example.\n",
    "\n",
    "### Parameters\n",
    "- **Array size**: `n = 8` (2³)\n",
    "- **Memory banks**: 2 (2¹)\n",
    "- **Bank mapping**: Element `x[i]` → Bank `(i mod 2)`\n",
    "  - Bank 0: `x[0], x[2], x[4], x[6]`\n",
    "  - Bank 1: `x[1], x[3], x[5], x[7]`\n",
    "\n",
    "\n",
    "## Recursive doubling Analysis\n",
    "**Algorithm Structure**:\n",
    "```python\n",
    "for s in [2, 4, 8]:\n",
    "    for i in steps of size s:\n",
    "        x[i] += x[i + s/2]\n",
    "```\n",
    "\n",
    "### Key Iterations:\n",
    "1. **s = 2** (Stride = 1):\n",
    "   - Operations: `(x[0] += x[1]), (x[2] += x[3]), (x[4] += x[5]), (x[6] += x[7])`\n",
    "   - **Bank Access**:\n",
    "     - Each pair accesses both banks (e.g., `x[0]` (Bank 0) + `x[1]` (Bank 1)).\n",
    "     - **Conflict**: All 4 operations compete for both banks, requiring full serialization.\n",
    "   - **Parallel Efficiency**: 50% (2 banks, 4 operations).\n",
    "\n",
    "2. **s = 4** (Stride = 2):\n",
    "   - Operations: `(x[0] += x[2]), (x[4] += x[6])`\n",
    "   - **Bank Access**:\n",
    "     - Both operations target Bank 0 exclusively.\n",
    "     - **Conflict**: Full serialization required for Bank 0.\n",
    "   - **Parallel Efficiency**: 0% (no parallelism possible).\n",
    "\n",
    "3. **s = 8** (Stride = 4):\n",
    "   - Operation: `(x[0] += x[4])`\n",
    "   - **Bank Access**:\n",
    "     - Both indices belong to Bank 0.\n",
    "   - **Conflict**: Single-threaded execution.\n",
    "   - **Parallel Efficiency**: 0%.\n",
    "\n",
    "**Summary**:\n",
    "- Parallel efficiency degrades from **50% → 0%**.\n",
    "- Critical final iterations suffer from severe bank conflicts.\n",
    "\n",
    "## Recursive Halving Analysis\n",
    "**Algorithm Structure**:\n",
    "```python\n",
    "for s in [4, 2, 1]:\n",
    "    for all i in parallel:\n",
    "        x[i] += x[i + s]\n",
    "```\n",
    "\n",
    "### Key Iterations:\n",
    "1. **s = 4** (Stride = 4):\n",
    "   - Operations: All threads access `(x[i], x[i+4])`.\n",
    "   - **Bank Access**:\n",
    "     - Even `i` → Bank 0 (e.g., `x[0], x[4]`).\n",
    "     - Odd `i` → Bank 1 (e.g., `x[1], x[5]`).\n",
    "   - **Conflict**: 4 threads/bank, but conflicts distributed across both banks.\n",
    "   - **Parallel Efficiency**: 50% (2 banks, 8 operations).\n",
    "\n",
    "2. **s = 2** (Stride = 2):\n",
    "   - Operations: All threads access `(x[i], x[i+2])`.\n",
    "   - **Bank Access**:\n",
    "     - Even `i` → Bank 0 (e.g., `x[0], x[2]`).\n",
    "     - Odd `i` → Bank 1 (e.g., `x[1], x[3]`).\n",
    "   - **Conflict**: 4 threads/bank, but reduced contention due to smaller strides.\n",
    "   - **Parallel Efficiency**: 75%.\n",
    "\n",
    "3. **s = 1** (Stride = 1):\n",
    "   - Operations: All threads access `(x[i], x[i+1])`.\n",
    "   - **Bank Access**:\n",
    "     - Alternating banks (e.g., `x[0]` (Bank 0) + `x[1]` (Bank 1)).\n",
    "   - **Conflict**: None (perfect bank interleaving).\n",
    "   - **Parallel Efficiency**: 100%.\n",
    "\n",
    "**Summary**:\n",
    "- Parallel efficiency improves from **50% → 100%**.\n",
    "- Final critical iteration achieves maximal parallelism.\n",
    "\n",
    "## Comparative Analysis\n",
    "\n",
    "| **Metric**               | **Recursive Doubling**           | **Recursive Halving**            |\n",
    "|--------------------------|-----------------------------------|-----------------------------------|\n",
    "| **Bank Conflict Trend**  | Conflicts increase with stride    | Conflicts decrease with stride    |\n",
    "| **Critical Phase**       | Final iteration (0% efficiency)   | Final iteration (100% efficiency) |\n",
    "| **Parallel Scalability** | Degrades as algorithm progresses  | Improves as algorithm progresses  |\n",
    "| **Conflict Distribution**| Concentrated on one bank          | Distributed across both banks     |\n",
    "\n",
    "### Key Advantages of Recursive Halving:\n",
    "1. **Final-Phase Optimization**: Perfect bank distribution in the critical final iteration maximizes parallelism when results are needed.\n",
    "2. **Early Conflict Overlap**: Conflicts in early phases are less impactful and can be hidden behind computations.\n",
    "3. **Scalable Efficiency**: Parallel efficiency increases as the algorithm progresses, matching the urgency of later phases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.14\n",
    "\n",
    "The matrix-matrix product, considered as `operation`, clearly has data reuse by the above definition. Argue that this reuse is not trivially attained by a simple implementation. What determines whether the naive implementation has reuse of data that is in cache?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The naive implementation of matrix-matrix multiplication:\n",
    "```c\n",
    "for (i = 0; i < n; i++) {\n",
    "    for (j = 0; j < m; j++) {\n",
    "        for (l = 0; l < k; l++) {\n",
    "            C[i][j] += A[i][l] * B[l][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "does not trivially exploit **data reuse** in cache. The issue lies in the access patterns of the matrices, particularly \\( B \\), leading to frequent cache misses and inefficient reuse.\n",
    "\n",
    "\n",
    "### **Why Naive Implementation Fails to Exploit Reuse**\n",
    "1. **Matrix \\( A \\):**\n",
    "   - Accessed row-wise (\\( A[i][l] \\)).\n",
    "   - Spatial reuse is achieved within a single row, but not across rows or columns.\n",
    "\n",
    "2. **Matrix \\( B \\):**\n",
    "   - Accessed column-wise (\\( B[l][j] \\)).\n",
    "   - Poor spatial locality in row-major storage; data is scattered in memory, causing frequent cache misses.\n",
    "\n",
    "3. **Matrix \\( C \\):**\n",
    "   - Accessed row-wise (\\( C[i][j] \\)).\n",
    "   - Updates occur sequentially, which is efficient if the matrix fits in cache.\n",
    "\n",
    "\n",
    "### **Solution: Optimize for Cache Reuse**\n",
    "To achieve efficient data reuse in cache, we can:\n",
    "\n",
    "#### **1. Blocking (Tiling)**\n",
    "Break matrices into smaller submatrices (blocks) that fit into cache, ensuring all accesses within the block exploit spatial and temporal locality.\n",
    "\n",
    "**Implementation:**\n",
    "```c\n",
    "for (ii = 0; ii < n; ii += block_size) {\n",
    "    for (jj = 0; jj < m; jj += block_size) {\n",
    "        for (ll = 0; ll < k; ll += block_size) {\n",
    "            for (i = ii; i < ii + block_size; i++) {\n",
    "                for (j = jj; j < jj + block_size; j++) {\n",
    "                    for (l = ll; l < ll + block_size; l++) {\n",
    "                        C[i][j] += A[i][l] * B[l][j];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**How It Works:**\n",
    "- Each block of \\( A \\), \\( B \\), and \\( C \\) is small enough to fit into cache.\n",
    "- **Reuse:**\n",
    "  - \\( A[i][l] \\): Reused across multiple columns of \\( B \\).\n",
    "  - \\( B[l][j] \\): Reused across multiple rows of \\( A \\).\n",
    "  - \\( C[i][j] \\): Remains in cache during updates.\n",
    "\n",
    "\n",
    "#### **2. Loop Reordering**\n",
    "Reorder loops to improve spatial locality, especially for \\( B \\). For example:\n",
    "```c\n",
    "for (i = 0; i < n; i++) {\n",
    "    for (l = 0; l < k; l++) {\n",
    "        for (j = 0; j < m; j++) {\n",
    "            C[i][j] += A[i][l] * B[l][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Improvement:**\n",
    "- Iterating over \\( j \\) in the innermost loop makes \\( B[l][j] \\) accesses sequential, improving spatial locality.\n",
    "\n",
    "\n",
    "#### **3. Transposing \\( B \\)**\n",
    "Transpose \\( B \\) to convert column-wise access into row-wise access:\n",
    "\\[\n",
    "B'[j][l] = B[l][j]\n",
    "\\]\n",
    "\n",
    "**Modified Code:**\n",
    "```c\n",
    "for (i = 0; i < n; i++) {\n",
    "    for (j = 0; j < m; j++) {\n",
    "        for (l = 0; l < k; l++) {\n",
    "            C[i][j] += A[i][l] * B'[j][l];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Improvement:**\n",
    "- Accessing \\( B'[j][l] \\) is now sequential in row-major order, significantly reducing cache misses.\n",
    "\n",
    "\n",
    "#### **Key Factors That Determine Cache Reuse**\n",
    "1. **Cache Size:**\n",
    "   - The working set (active submatrices of \\( A \\), \\( B \\), and \\( C \\)) must fit into cache.\n",
    "\n",
    "2. **Access Patterns:**\n",
    "   - Sequential access (spatial locality) reduces cache misses.\n",
    "   - Blocking ensures reuse by limiting the working set to small tiles.\n",
    "\n",
    "3. **Matrix Layout:**\n",
    "   - Transposing \\( B \\) ensures compatibility with row-major storage.\n",
    "\n",
    "\n",
    "### **Conclusion**\n",
    "To exploit cache reuse efficiently in matrix-matrix multiplication:\n",
    "1. Use **blocking** to process small submatrices that fit in cache.\n",
    "2. Reorder loops to improve access patterns for \\( B \\).\n",
    "3. Transpose \\( B \\) to align its access pattern with row-major storage.\n",
    "\n",
    "These optimizations minimize cache misses and significantly improve performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.16\n",
    "\n",
    "Consider the following pseudocode of an algorithm for summing `n` numbers `x[i]` where `n` is a power of $2$:\n",
    "\n",
    "```pseudo\n",
    "for s=2,4,8,...,n/2,n:\n",
    "    for i=0 to n-1 with steps s:\n",
    "        x[i] = x[i] + x[i+s/2]\n",
    "sum = x[0]\n",
    "```  \n",
    "Analyze the spatial and temporal locality of this algorithm, and contrast it with the standard algorithm, \n",
    "\n",
    "```pseudo\n",
    "sum = 0\n",
    "for i=0,1,2,...,n-1\n",
    "    sum = sum + x[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex's Solution\n",
    "\n",
    "First, here are some quick definitions:\n",
    "* **Temporal Locality:** If a memory location is accessed once, then it's likely to be accessed again soon. This benefits from cache reuse, where recently used data remains in \"fast memory\" (e.g. the cache).\n",
    "* **Spatial Locality:** If a memory location is accessed, nearby locations are likely to be acessed soon. This benefits from cache lines which load chunks of adjacent memory at once. \n",
    "\n",
    "**First Algorithm (Parallelizable Summation)**: \n",
    "* **Temporal Locality:** In the first iteration with `s=2`, every even-indexed element of `x[i]` is updated using it's neighboring odd-indexed element `x[i] + x[i+1]`. So, each element is accesed exactly once, meaning there's no intra-iteration temporal locality since values aren't re-used in the same outer loop step. Additionally, as `s` increases in subsequent iterations, the previously computed values are accessed at increasingly larger intervals. This could introduce some temporal locality across iterations since values computed in earlier steps are accessed again in later ones, however, since `s` is growing exponentially, the gap between these accesses increases, making it less likely that these elements will still be in cache. Finally, since the `sum` term does not appear inside the loop and is only accessed once, it doesn't affect the temportal locality. Ultimately, there is no noticable temporal locality. \n",
    "* **Spatial Locality:** Consecutive elements (ie. `x[i]` and `x[i + s/2]`) are accessed together, showing spatial locality by taking advantage of cache lines (fixed-size blocks of memory, typically 32-64 bytes). This allows multiple adjacent elements to be loaded into the cache at once, showing good spatial locality. However, as `s` increases exponentially in powers of 2, memory accesses become strided, meaning elements are accessed with gaps. This reduces spatial locality because fewer elements fit in the same cache line, causing the CPU to need to load new cache lines more frequently. Again, since the `sum` term is only accessed at the end of the algorithm, it doesn't impact memory access patterns during the loop. Ultimately, there is no noticable spatial locality. \n",
    "\n",
    "**Second Algorithm (Standard Summation):** \n",
    "* **Temporal Locality:** Since the sum variable is updated in every iteration and is repeatedly accessed throughout the loop, this exhibits strong temporal locality as the sum is likely stored in cache. \n",
    "* **Spatial Locality:** Since the elements sof `x[i]` are accesses sequentially, there is strong spatial locality as accessing consecutive elements in memory allows them to be loaded in the same cache line. These predictable memory accessing also help avoid cache misses. \n",
    "\n",
    "In conclusion, the second algorithm is well-suited for single core execution because at each iteration, the new value of `sum` is computed based on it's previous value, forming a data dependency chain. While this algorithm is inherently seriel, it could still be parallalized... once such method might be to use Parallel Reduction (ie. splitting the array into smaller chunks and sum each chunk in parallel, combining results at the end). However, the first algorithm, which updates `x[i] = x[i] + x[i + s/2]`, involves independent operations where the updates for different indices are not dependent on each other, making it very parallelizable since each update can be performed simultaneously on different processors. For example, when `s=2`, multiple updates like `x[0] = x[0] + x[1]`, `x[2] = x[1] + x[3]`, etc., can occur in parallel across different processors. Again, as `s` increases, these operations become less adjacent, but they remain independent at each step. Thus, this algorth is well-suited for multi-core execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.17\n",
    "Consider the following code, and assume that `nvectors` is small compared to the cache size, and length large.\n",
    "\n",
    "```pseudo\n",
    "for (k=0; k<nvectors; k++) \n",
    "    for (i=0; i<length; i++)\n",
    "        a[k,i] = b[i] * c[k]\n",
    "```\n",
    "\n",
    "How do the following concepts relate to the performance of this code:\n",
    "* Reuse\n",
    "* Cache Size\n",
    "* Associativity\n",
    "\n",
    "Would the following code where the loops are exchanged perform better or worse, and why?\n",
    "\n",
    "```pseudo\n",
    "for (i=0; i<length; i++) \n",
    "    for (k=0; k<nvectors; k++)\n",
    "        a[k,i] = b[i] * c[k]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex's Solution\n",
    "\n",
    "First, here are some quick definitions: \n",
    "* **Reuse:** repeated access of the same data within a short period which can improve perforance by keeping frequently used data in cache. This definition can be further split into \"temporal use\" (same data accessed multiple times) and \"spatial use\" (nearby data accessed becuase of memory locality). \n",
    "* **Cache Size:** the amount of memory available in the cache to store frequently used data. These are often very fast, but can hold very limited amounts of data. \n",
    "* **Associativity:** the number of places a block of memory can be stored within a cache set. Higher associativity allows a memory block to be placed in multiple locations within the cahce, reducing cache conflicts (when multiple memory addresses compete for the same cache set) and improving cache hit rates. In contrast, lower associativity means that each cache set has fewer ways a memory block could be stored, which leads to a bigger risk of cache conflicts wen multiple memory addresses map to the same set.  \n",
    "\n",
    "Analysis of the First Code: \n",
    "* **Reuse:** \n",
    "    * `b[i]` is sequentially accessed in the inner loop, meaning it has good temporal reuse as it remains in the cache while iterating over `k`. \n",
    "    * `c[k]` would have poor reuse becuase it's only accessed once per iteration of `i`, however, due to the small size of `nvectors`, we can assume that `c[k]` likely remains in cache without frequent evictions. \n",
    "* **Cache Size:** \n",
    "    * Since `nvectors` is taken to be very small, then we can claim that both `c[k]` and `a[k,i]` are likely to fit in the cache.\n",
    "    * `b[i]` benefits from sequential access and prefetching, which reduces cache misses. This loop ordering allows `b[i]` to stay in cache for longer, improving performance. \n",
    "* **Associativity:** \n",
    "    * Accessing `a[k,i]` across different `k` values could lead to cache conflicts if multiple elemnts of `a[k,i]` map to the same cache set.\n",
    "    * Since the elements of `a[k,i]` are accessed row-wise in row-major order (this is the storage method that common language, like Python and C++, use to store multi-dimensional arrays), then spatial locality would be optimized. \n",
    "\n",
    "If we swapped the loop order, as is done in the second code, the performance would worsen. In this version of code, `c[k]` would now be accessed in the inner loop, meaning it is repeatedly loaded into cache for each `i`, reducing temporal resuse and increasing cache misses. Additionally, this version would access `a[k,i]` column-wise, which might cause conflicts misses in a low-associativity cache if multiple rows maps to the same cache set and evict eachother. Overall, the original loop ordering performs better because it optimizes memory access patterns, reduces cache conflicts, and takes advantage of sequential memory access.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "While the strategy just sketched will demonstrate the existence of cache sizes, it will not report the maximal bandwidth that the cache supports. What is the problem and how would you fix it ?\n",
    "\n",
    "---\n",
    "**Answer** : The problem arises from random memory access. Since these accesses are scattered, they do not fit efficiently into a cache line, leading to inefficient use of the cache. To maximize bandwidth, data should be accessed sequentially, ensuring that entire cache lines are utilized. To prevent loop swapping, we can disable compiler optimizations or introduce dependencies between iterations : \n",
    "\n",
    "```C\n",
    "int memory[SIZE] = {0};\n",
    "for (int irepeat = 0; irepeat < how_many_repeats; irepeat++) {\n",
    "    memory[0] = random()% irepeat;\n",
    "    memory[1] = random()% irepeat;\n",
    "    for (int iword = 2; iword < SIZE; iword++) \n",
    "        memory[iword] += memory[iword-1] + memory[iword-2];\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an example of a doubly nested loop where the loops can be exchanged; give an example where this can not be done. If at all possible, use practical examples from this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a doubly nested loop where the loops can be exchanged is on page 271 in the textbook:\n",
    "\n",
    "```\n",
    "for (j=0; j<n;j++) {\n",
    "    for (i=0; i<m; i++) {\n",
    "        array[INDEX(i,j,m,n)] = array[INDEX(i,j,m,n)]+1;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inexchangable loop would be one where the inner loop depends somehow on the outer one, a good example of this can be found on page 275: \n",
    "```\n",
    "bs = ...\n",
    "nblocks = n/bs\n",
    "for (b=0; b<nblocks; b++) {\n",
    "    for (i=b*bs,j=0; j<bs; i++,j++) {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you come up with at least two reasons why this is better for performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first assumption was that it had something to do with the way the languages compiled to assembly, but that is not the case. It is actually because of the way arrays are stored in the cache by the two languages\n",
    "\n",
    "1. Multidimensional arrays are stored 1 dimensionally. The way C would store an array like: $x=\\begin{bmatrix}a & b\\\\c & d\\end{bmatrix}$ like this: $[a,b,c,d]$. The way that indexing this array would be: $x[column (i)][row (j)]$. If you have j index be in the inner loop, you can get these elements in order, which is more efficient than having the code jump all over the place. This means that C is row major. Fortran stores its data differently, it is column major. This means that looping with the i index is better for that language.\n",
    "2. Compilers optimize loops using loop unrolling and these things work better if the data is accessed sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze this example. When is x brought into cache, when is it reused, and when is it flushed? What is the required cache size in this example? Rewrite this example, using a constant ```define L1SIZE 65536```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the example the problem is referring to is the block of code directly above it.\n",
    "\n",
    "So, I think that x is too large for all of it to fit in the cache. This means that portions of the array will be brought in and deleted as things are loaded. This is done n times for the outer loop. But, with loop tiling, x with be retained in the cache, and this won't happen at all. If x consists of int elements then the block size should be L1SIZE/sizeof(int)\n",
    "\n",
    "Code below:\n",
    "```\n",
    "include stdio.h\n",
    "\n",
    "define L1SIZE 65536\n",
    "define DATASIZE 100000\n",
    "\n",
    "int main() {\n",
    "    double x[DATASIZE]\n",
    "    int bs = L1SIZE/sizeof(double);\n",
    "    int nblocks = DATASIZE/bs;\n",
    "    int b, n, i;\n",
    "\n",
    "    for (b = 0; b < nblocks; b++) {\n",
    "        for (n = 0; n < ITERATIONS; n++) {\n",
    "            for (i = b * bs; i < (b + 1) * bs; i++) {\n",
    "                x[i] += n * 1.0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
