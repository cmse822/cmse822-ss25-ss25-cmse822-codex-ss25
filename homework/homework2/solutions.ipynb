{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2\n",
    "\n",
    "Group: Thread-Titans\n",
    "\n",
    "Group Members: Alex, Bashar, Fizza, Maya, Ramin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.6\n",
    "Prove that $ùê∏ = 1$ implies that all processors are active all the time.\n",
    "(Hint: suppose all processors finish their work in time $T$ , except for one processor in $T'$ < $T$ . What is $T_p$ in this case? Explore the above relations.)\n",
    "\n",
    "\n",
    "To prove that $E = 1$ implies that all processors are active all the time, we start with the definition of efficiency:\n",
    "\n",
    "$$E_p = \\frac{S_p}{p} = \\frac{T_1 / T_p}{p}$$\n",
    "\n",
    "If $E_p = 1$, then:\n",
    "\n",
    "$$\\frac{T_1 / T_p}{p} = 1$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$T_p = \\frac{T_1}{p}$$\n",
    "\n",
    "This is the ideal case where speedup $S_p$ is equal to $p$, i.e, the execution time is perfectly divided among the $p$ processors.\n",
    "\n",
    "On the contrary, assume that one processor finishes its work earlier, at time $T' < T$, while others continue working. The total execution time $T_p$ would then be determined by the slowest processor, which still takes $T$. However, since one processor was idle for duration $T - T'$, this means all processors (atleast there is 1) were not fully utilized.\n",
    "\n",
    "If $E_p = 1$, then there is no such idle time and every processor is doing useful work at every moment until completion. Therefore, all processors must be active the entire time.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.10\n",
    "\n",
    "Show that, with the scheme for parallel addition just outlined, you can multiply two matrices in $\\log_2 N$ time with $N^3/2$ processors. What is the resulting efficiency?\n",
    "\n",
    "The standard matrix multiplication algorithm of  two $N \\times N$ matrices requires $O(N^3)$ operations. Each element $C_{ij}$ in the result matrix is computed as:\n",
    "\n",
    "$$C_{ij} = \\sum_{k=1}^{N} A_{ik} B_{kj}$$\n",
    "\n",
    "which involves summing $N$ terms. Instead of summing sequentially in $O(N)$ time, we use the parallel sum strategy: each processor starts with a single term, then in each step, pairs of values are summed in parallel. Since the number of active processors halves at each step, the total number of steps is $\\log_2 N$. Thus, summing $N$ terms takes $O(\\log_2 N)$ time.\n",
    "\n",
    "To determine why we use $N^3/2$ processors, note that matrix multiplication involves computing all $N^2$ elements of $C$. Each element requires $N$ multiplications followed by $N-1$ additions, giving a total of approximately $N^3$ operations. To parallelize this efficiently, we assign one processor per multiplication, which requires $N^3$ processors. However, summation follows a binary tree structure, meaning that summing two values requires one processor for each pair. Since there are $N^3$ terms and each step halves the number of terms, we only need about $N^3/2$ processors for summation. Thus, with $N^3/2$ processors, we can fully parallelize both multiplication and summation.\n",
    "\n",
    "Since the slowest step in this approach is summation, and we use the parallel sum strategy, the total execution time for matrix multiplication is $O(\\log_2 N)$. Efficiency is given by:\n",
    "\n",
    "$$E = \\frac{\\text{sequential time}}{\\text{parallel time} \\times \\text{number of processors}}$$\n",
    "\n",
    "Since sequential matrix multiplication takes $O(N^3)$ time, our parallel version runs in $O(\\log_2 N)$ time with $O(N^3/2)$ processors. This gives:\n",
    "\n",
    "$$E = \\frac{O(N^3)}{O(\\log_2 N) \\times O(N^3/2)} = O\\left(\\frac{N^3}{(N^3/2) \\log_2 N}\\right) = O\\left(\\frac{2}{\\log_2 N}\\right)$$\n",
    "\n",
    "This efficiency decreases as $N$ grows but remains better than many naive parallel approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.11\n",
    "\n",
    "Let‚Äôs do a specific example. Assume that a code has a setup that takes 1 second and a parallelizable section that takes 1000 seconds on one processor. What are the speedup and efficiency if the code is executed with 100 processors? What are they for 500 processors? Express your answer to at most two significant digits.\n",
    "\n",
    "\n",
    "Let the total execution time on one processor be $T_1 = 1 + 1000 = 1001$ seconds. The sequential fraction of the code is $F_s = \\frac{1}{1001}$, and the parallelizable fraction is $F_p = \\frac{1000}{1001}$. To compute the speedup and efficiency for 100 processors, we use Amdahl‚Äôs Law: \n",
    "\n",
    "$$ T_P = T_1(F_s + F_p/P)$$\n",
    "$$ T_{100} = 1001 \\left( \\frac{1}{1001} + \\frac{1000}{1001 \\times 100} \\right) = 11$$\n",
    "\n",
    "The speedup is:\n",
    "\n",
    "$$ S_{100} = \\frac{T_1}{T_{100}} = \\frac{1001}{11} = 91$$\n",
    "\n",
    "and the efficiency is:\n",
    "\n",
    "$$ E_{100} = \\frac{S_{100}}{p} = \\frac{91}{100} = 0.91$$\n",
    "\n",
    "For 500 processors, we similarly compute:\n",
    "\n",
    "$$ T_{500} = 1001 \\left( \\frac{1}{1001} + \\frac{1000}{1001 \\times 500} \\right) = 3$$\n",
    "\n",
    "The speedup is:\n",
    "\n",
    "$$ S_{500} = \\frac{1001}{3} = 333.6667 $$\n",
    "\n",
    "and the efficiency is:\n",
    "\n",
    "$$ E_{500} = \\frac{333.6667}{500} = 0.6673334$$\n",
    "\n",
    "Using the above formulas, we find that for 100 processors, the speedup is approximately 91 and the efficiency is 0.91, while for 500 processors, the speedup is approximately 334 and the efficiency is 0.67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.12\n",
    "\n",
    "Investigate the implications of Amdahl‚Äôs law: if the number of processors ùëÉ increases, how does the parallel fraction of a code have to increase to maintain a fixed\n",
    "efficiency?\n",
    "\n",
    "Consider the speedup and efficiency of a parallel system given by the Amdahl law:\n",
    "\n",
    "$$ S_P = \\frac{1}{F_s + \\frac{F_p}{P}} $$\n",
    "\n",
    "The efficiency $E_P$ is defined as:\n",
    "\n",
    "$$ E_P = \\frac{S_P}{P} = \\frac{1}{P \\left( F_s + \\frac{F_p}{P} \\right)} $$\n",
    "\n",
    "To maintain a fixed efficiency as $P$ increases, we have to keep the denominator constant. That can only be made possible if the parallel fraction $F_p$ increases as $P$ grows. \n",
    "\n",
    "For large $P$, the term $\\frac{F_p}{P}$ becomes small compared to $F_s$ term, so the efficiency is primarily determined by $F_s$. Therefore, in order to maintain a constant efficiency, the parallel fraction $F_p$ must increase to offset the increasing effect of the sequential fraction $F_s$.\n",
    "\n",
    "In a nutshell if $P$ increases, the benefit of parallelization diminishes unless the parallel fraction $F_p$ also increases. If the sequential fraction $F_s$ remains constant, there is a limit to the speedup, and the efficiency will decrease as more processors are added. To maintain the same efficiency, the parallelizable part of the code must grow in proportion to $P$ to overcome the growing impact of the sequential part.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
